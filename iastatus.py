
text/x-generic passenger_wsgi.py ( Python script, UTF-8 Unicode text executable )
# -- coding: utf-8 --
"""Proyecto_Dashboard_completo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xfIRI5WI-GMiY-Gw_eaVQJx4ZeAKpzuY
"""

# Commented out IPython magic to ensure Python compatibility.

# %pip install PyICU pycld2 Morfessor datasets chardet polyglot textblob requests wordcloud transformers matplotlib seaborn psycopg2



import os
os.environ["OPENBLAS_NUM_THREADS"] = "4"
import argparse
import requests
import pandas as pd
import re
import unicodedata
import nltk
nltk.download('punkt_tab')
print(nltk.data.path)  
from textblob import TextBlob
import matplotlib.pyplot as plt
import seaborn as sns

from nltk.stem.snowball import SnowballStemmer
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from transformers import pipeline
#sentiment_pipeline = pipeline('sentiment-analysis', model='nlptown/bert-base-multilingual-uncased-sentiment')

import transformers
import torch
torch.set_num_threads(1)
transformers.logging.set_verbosity_error()  # Opcional: Para reducir el ruido de los logs

if torch.cuda.is_available():
    torch.cuda.empty_cache()

sentiment_pipeline = pipeline('sentiment-analysis', model='nlptown/bert-base-multilingual-uncased-sentiment', framework='pt', device=-1)




import psycopg2
from collections import OrderedDict  # Importación correcta de OrderedDict

nltk.download('stopwords')
nltk.download('punkt')
nltk.download('movie_reviews')
nltk.download('vader_lexicon')

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# Definición de funciones de preprocesamiento
nltk_stopwords = set(stopwords.words('spanish'))

def preprocessing(dataframe, fieldName, config):
    def apply_preprocessing(text):
        if config["lowercase"]:
            text = text.lower()
        if config["remove_accentuation"]:
            text = unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('ASCII')
        if config["remove_punctuation"]:
            text = re.sub(r'[^\w\s]', '', text)
        if config["remove_others"]:
            text = re.sub(r'[^\w\s]', '', text)
        if config["remove_numbers"]:
            text = re.sub(r'\d+', '', text)
        if config["remove_stopwords"]:
            text = " ".join(word for word in text.split() if word not in nltk_stopwords)
        if config["stemming"]:
            text = " ".join(SnowballStemmer("spanish").stem(word) for word in text.split())
        if config["remove_emojis"]:
            text = text.encode('ascii', 'ignore').decode('ascii')
        return text

    return [apply_preprocessing(tx) for tx in dataframe[fieldName]]

# Función para transformar los datos
def transformData(data, fieldName, my_tokenizer, weight="TFIDF"):
    vectorizer_dict = {
        "TP": CountVectorizer(tokenizer=my_tokenizer, binary=True),
        "TF": CountVectorizer(tokenizer=my_tokenizer),
        "TFIDF": TfidfVectorizer(tokenizer=my_tokenizer)
    }
    vectorizer = vectorizer_dict.get(weight, TfidfVectorizer(tokenizer=my_tokenizer))
    X = vectorizer.fit_transform(data[fieldName])
    return vectorizer, X

# Función para análisis de temas (LDA)
def LDA(bow, topics, n_top_words):
    feature_names = list(bow)
    model = LatentDirichletAllocation(n_components=topics, max_iter=10, learning_method='online', learning_decay=0.9, random_state=4)
    model.fit(bow)

    for index, topic in enumerate(model.components_):
        message = f"\nTopic #{index}: " + " ".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])
        print(message)
        print("="*70)

# Función para análisis de sentimiento
def analyze_sentiment(data, sentiment_pipeline):
    polarity = []
    for dat in data:
        try:
            result = sentiment_pipeline(dat)[0]
            label = result['label']
            if '1 star' in label or '2 stars' in label:
                polarity.append(-1)
            elif '4 stars' in label or '5 stars' in label:
                polarity.append(1)
            else:
                polarity.append(0)
        except Exception as e:
            print(f"Error al procesar el texto: {dat}. Error: {e}")
            polarity.append(0)
    return pd.DataFrame({'text': data, 'polarity': polarity})

# Configuración para el preprocesamiento
config = {
    "lowercase": True,
    "remove_accentuation": True,
    "remove_punctuation": True,
    "remove_others": True,
    "remove_numbers": True,
    "remove_stopwords": True,
    "stemming": False,
    "remove_emojis": True,
}
# Definir y analizar argumentos de la línea de comandos
parser = argparse.ArgumentParser(description='Script de análisis de sentimientos y temas')
parser.add_argument('--date_start', type=str, required=True, help='Fecha de inicio para el análisis')
parser.add_argument('--date_end', type=str, required=True, help='Fecha de fin para el análisis')
args = parser.parse_args()

date_start = args.date_start
date_end = args.date_end
# Realizar la solicitud POST para obtener los comentarios
api_url = "https://reportapi.infocenterlatam.com/api/fstadistic/getReportListenComments"
payload = {
    "date_start": date_start,
    "date_end": date_end
}
print(payload)  # Esto debería mostrar el payload antes de enviarlo
response = requests.post(api_url, json=payload)

if response.status_code == 200:
    data_json = response.json()

    if data_json.get("status") == True:

        # Extraer el ID de "data"
        id_value = data_json["data"].get("id", "default_id_value")  # Ajusta la clave según la estructura de la respuesta

        comments = data_json["data"].get("comments", [])
        messages = [comment.get("message", "") for comment in comments]
        comments_df = pd.DataFrame({'comments': messages})

        # Preprocesar los comentarios
        comments_pp = pd.DataFrame({'comments': preprocessing(comments_df, fieldName='comments', config=config)})
        print("Comentarios preprocesados:")
        print(comments_pp.head())

        # Transformar los datos para modelado de temas
        vectorizer, X = transformData(comments_pp, fieldName="comments", my_tokenizer=word_tokenize, weight="TFIDF")
        feature_names = vectorizer.get_feature_names_out()
        text_collection = OrderedDict([(index, text) for index, text in enumerate(comments_pp["comments"])])
        corpus_index = [n for n in text_collection]
        bow = pd.DataFrame(X.todense(), index=corpus_index, columns=feature_names)

        # Realizar el modelado de temas (LDA)
        LDA(bow, 5, 10)

        # Análisis de sentimiento utilizando un modelo BERT
        sentiment_pipeline = pipeline('sentiment-analysis', model='nlptown/bert-base-multilingual-uncased-sentiment')
        dfSenti = analyze_sentiment(comments_pp.comments.tolist(), sentiment_pipeline)
        # Clasificación de sentimientos
        dataLabel = dfSenti.polarity.tolist()
        negative = sum(1 for p in dataLabel if p == -1)
        neutral = sum(1 for p in dataLabel if p == 0)
        positive = sum(1 for p in dataLabel if p == 1)

        print("Negative Polarity: ", negative)
        print("Neutral Polarity: ", neutral)
        print("Positive Polarity: ", positive)

        # Conexión a la base de datos PostgreSQL e inserción de datos
        print("id_post: ", id_value)
        api_url = f"https://reportapi.infocenterlatam.com/api/fstadistic/iasetResponse/{id_value}"
        payload = {
            "ia_positive":positive,
            "ia_negative":negative,
            "ia_neutro":neutral
        }
        # Realizar la solicitud POST
        response = requests.post(api_url, json=payload)

        # Verificar la respuesta
        if response.status_code == 200:
           print("Datos enviados exitosamente.")
           print("Respuesta del servidor:", response.json())
        else:
            print(f"Error al enviar los datos. Código de estado: {response.status_code}")
            print("Mensaje del servidor:", response.text)

    else:
        print("Error en la consulta: ", data_json.get("message"))
else:
    print(f"Error en la solicitud: {response.status_code}")